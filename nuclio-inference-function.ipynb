{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuclio - Infer function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import v3io_frames as v3f\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df_from_tsdb(context, df):\n",
    "    df.index.names = ['timestamp', 'company', 'data_center', 'device']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_tsdb(context, df: pd.DataFrame):   \n",
    "    # Fix indexes before saving to TSDB\n",
    "    df = set_indexes(df)\n",
    "    # Save to TSDB\n",
    "    context.v3f.write('tsdb', context.predictions_table, df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_indexes(df):\n",
    "    df = df.set_index(['timestamp', 'company', 'data_center', 'device'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_parquet(context):\n",
    "    # Get parquet files\n",
    "    mpath = [os.path.join(context.features_table, file) for file in os.listdir(context.features_table)]\n",
    "    \n",
    "    # Get latest filename\n",
    "    latest = max(mpath, key=os.path.getmtime)\n",
    "    print(latest)\n",
    "    context.logger.debug(f'Reading data from: {latest}')\n",
    "    \n",
    "    # Load parquet to dask\n",
    "    df = pd.read_parquet(latest)\n",
    "    \n",
    "    # Keep columns\n",
    "    keep_columns = [col for col in df.columns if 'is_error' not in col]\n",
    "    \n",
    "    # Keep good columns and Sort them\n",
    "    df = df[sorted(keep_columns)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_parquet(context, df: pd.DataFrame):\n",
    "    print('Saving features to Parquet')\n",
    "    \n",
    "    # Need to fix timestamps from ns to ms if we write to parquet\n",
    "    df = df.reset_index()\n",
    "    df['timestamp'] = df.loc[:, 'timestamp'].astype('datetime64[ms]')\n",
    "    \n",
    "    # Fix indexes\n",
    "    df= set_indexes(df)\n",
    "    \n",
    "    # Save parquet\n",
    "    first_timestamp = df.index[0][0].strftime('%Y%m%dT%H%M%S')\n",
    "    last_timestamp = df.index[-1][0].strftime('%Y%m%dT%H%M%S')\n",
    "    filename = first_timestamp + '-' + last_timestamp + '.parquet'\n",
    "    filepath = os.path.join(context.predictions_table, filename)\n",
    "    with open(filepath, 'wb+') as f:\n",
    "        df.to_parquet(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_context(context):\n",
    "    \n",
    "    # Set Iguazio v3io connection\n",
    "    v3io_client = v3f.Client(address='framesd:8081', container='bigdata')\n",
    "    setattr(context, 'v3f', v3io_client)\n",
    "    \n",
    "    # Save features directory\n",
    "    features_table = os.path.join(os.getenv('NETAPP_MOUNT_PATH'),os.getenv('FEATURES_TABLE'))\n",
    "    setattr(context, 'features_table', features_table)\n",
    "    \n",
    "    # Save predictions directory\n",
    "    predictions_table = os.path.join('netops/',os.getenv('PREDICTIONS_TABLE'))\n",
    "    setattr(context, 'predictions_table', predictions_table)\n",
    "    \n",
    "    # Create predictions table if neede\n",
    "    context.v3f.create('tsdb', context.predictions_table, rate='1/s', if_exists=1)\n",
    "    \n",
    "    # Get saving configuration\n",
    "    is_from_tsdb = (int(os.getenv('FROM_TSDB', 1)) == 1)\n",
    "    \n",
    "\n",
    "    # Save to Parquet\n",
    "\n",
    "    # Create saving directory if needed\n",
    "    filepath = os.path.join(context.predictions_table)\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "\n",
    "    # Set Parquet reading function\n",
    "    setattr(context, 'read', get_data_parquet)\n",
    "\n",
    "    # Set Parquet saving function\n",
    "    setattr(context, 'write', save_to_tsdb)\n",
    "\n",
    "    # Load the model\n",
    "    model_path = os.path.join(os.getenv('APP_DIR'),os.getenv('SAVE_TO'),os.getenv('MODEL_FILENAME'))\n",
    "\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    setattr(context, 'model', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler(context, event):\n",
    "\n",
    "    # Load last hour data\n",
    "    df = context.read(context)\n",
    "    \n",
    "    # limit for testing\n",
    "    df = df.head(2)\n",
    "    \n",
    "    # Predict\n",
    "    df['prediction'] = context.model.predict(df.values)\n",
    "    \n",
    "    #print(df.head(1))\n",
    "    \n",
    "    # Prepare to save predictions\n",
    "    df = df.reset_index()\n",
    "    df = df.rename({'level_0': 'time',\n",
    "                    'level_1': 'company',\n",
    "                    'level_2': 'data_center',\n",
    "                    'level_3': 'device'}, axis=1)\n",
    "    \n",
    "    # Save\n",
    "    context.write(context, df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
