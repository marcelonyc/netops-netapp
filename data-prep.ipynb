{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "# DB Connection\n",
    "import v3io_frames as v3f\n",
    "\n",
    "# Parallelization\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "from mlrun import get_or_create_ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlrun] 2020-03-17 20:00:41,001 logging run results to: http://10.196.67.76:8080\n"
     ]
    }
   ],
   "source": [
    "mlruncontext = get_or_create_ctx('data-prep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dask client\n",
    "dask = Client()\n",
    "\n",
    "# Dask shards / CV\n",
    "shards =  int(mlruncontext.get_param('NUMBER_OF_SHARDS', 4))\n",
    "\n",
    "# Netops metrics table\n",
    "metrics_table = os.path.join(str(mlruncontext.get_input('DATA_DIR', os.getenv('DATA_DIR','/netpp'))),\n",
    "                             mlruncontext.get_param('metrics_table', os.getenv('metrics_table','netops_metrics_parquet')))\n",
    "\n",
    "# Netops feautres table\n",
    "features_table =  os.path.join(str(mlruncontext.get_param('NETAPP_MOUNT_PATH',os.getenv('NETAPP_MOUNT_PATH','/netapp'))),\n",
    "                               mlruncontext.get_param('FEATURES_TABLE', os.getenv('FEATURES_TABLE','netops_features_parquet')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/netpp/netops_metrics\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create saving directory if needed\n",
    "filepath = os.path.join(features_table)\n",
    "if not os.path.exists(filepath):\n",
    "    os.makedirs(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df_from_tsdb(df):\n",
    "    df.index.names = ['timestamp', 'company', 'data_center', 'device']\n",
    "    df = df.reset_index()\n",
    "    df = dd.from_pandas(df, npartitions=shards)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_parquet():\n",
    "    # Get parquet files\n",
    "    mpath = [os.path.join(metrics_table, file) for file in os.listdir(metrics_table)]\n",
    "    \n",
    "    # Get latest filename\n",
    "    latest = max(mpath, key=os.path.getmtime)\n",
    "    \n",
    "    # Load parquet\n",
    "    df = pd.read_parquet(latest)\n",
    "    # To Dask\n",
    "    df = format_df_from_tsdb(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rolling_featuers(df, window_size: int):\n",
    "    features = df.copy()\n",
    "    features['key'] = features.apply(lambda row: f'{row[\"company\"]}_{row[\"data_center\"]}_{row[\"device\"]}', axis=1, meta=features.compute().dtypes)\n",
    "    features.set_index('key')\n",
    "    features[\"cpu_utilization\"] = features.cpu_utilization.rolling(window=window_size).mean()\n",
    "    features[\"latency\"] = features.latency.rolling(window=window_size).mean()\n",
    "    features[\"packet_loss\"] = features.packet_loss.rolling(window=window_size).mean()\n",
    "    features[\"throughput\"] = features.throughput.rolling(window=window_size).mean()\n",
    "    features[\"is_error\"] = features.is_error.rolling(window=window_size).max()\n",
    "                                     \n",
    "    features = features.dropna()\n",
    "    features = features.drop_duplicates()\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_indexes(df):\n",
    "    df = df.set_index(['timestamp', 'company', 'data_center', 'device'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_parquet(df: pd.DataFrame):\n",
    "    print('Saving features to Parquet')\n",
    "    \n",
    "    # Need to fix timestamps from ns to ms if we write to parquet\n",
    "    df = df.reset_index()\n",
    "    df['timestamp'] = df.loc[:, 'timestamp'].astype('datetime64[ms]')\n",
    "    \n",
    "    # Fix indexes\n",
    "    df= set_indexes(df)\n",
    "    \n",
    "    # Save parquet\n",
    "    first_timestamp = df.index[0][0].strftime('%Y%m%dT%H%M%S')\n",
    "    last_timestamp = df.index[-1][0].strftime('%Y%m%dT%H%M%S')\n",
    "    filename = first_timestamp + '-' + last_timestamp + '.parquet'\n",
    "    filepath = os.path.join(features_table, filename)\n",
    "    with open(filepath, 'wb+') as f:\n",
    "        df.to_parquet(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler(context,event):\n",
    "    # Get data\n",
    "    raw = get_data_parquet() \n",
    "        \n",
    "    # Get minute features\n",
    "    minute = create_rolling_featuers(raw, 3)\n",
    "    column_names = {'cpu_utilization': 'cpu_utilization_minutely',\n",
    "                    'latency': 'latency_minutely',\n",
    "                    'packet_loss': 'packet_loss_minutely',\n",
    "                    'throughput': 'throughput_minutely'}\n",
    "    minute = minute.rename(columns=column_names)\n",
    "    \n",
    "    # Get hour features\n",
    "    hour = create_rolling_featuers(raw, 3*60)\n",
    "    column_names = {'cpu_utilization': 'cpu_utilization_hourly',\n",
    "                    'latency': 'latency_hourly',\n",
    "                    'packet_loss': 'packet_loss_hourly',\n",
    "                    'throughput': 'throughput_hourly'}\n",
    "    hour = hour.rename(columns=column_names)\n",
    "    \n",
    "    # Create feature vector from data sources\n",
    "    features_rm = raw.merge(minute, on=['timestamp', 'company', 'data_center', 'device'], suffixes=('_raw', '_minute'))\n",
    "    features_rm.compute()\n",
    "    \n",
    "    features = features_rm.merge(hour, on=['timestamp', 'company', 'data_center', 'device'], suffixes=('_raw', '_hourly'))\n",
    "    features = features.compute()\n",
    "    \n",
    "    # Save feature vector to TSDB\n",
    "    \n",
    "    # Drop key columns\n",
    "    features = features.reset_index(drop=True)\n",
    "    feature_cols = [col for col in features.columns if 'key' in col]\n",
    "    features = features.drop(feature_cols, axis=1)\n",
    "    \n",
    "    \n",
    "    # Fix indexes before saving\n",
    "    features = features.set_index(['timestamp', 'company', 'data_center', 'device'])\n",
    "    \n",
    "    # Save to TSDB\n",
    "    save_to_parquet(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
