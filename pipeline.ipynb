{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E2E Serverless ML pipeline  - Ingest, Train, Auto Deploy Model\n",
    "  --------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run set_env.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetApp volume mounts definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import new_function, code_to_function, NewTask, v3io_cred, new_model_server, mlconf, get_run_db, mount_v3io\n",
    "# for local DB path use 'User/mlrun' instead \n",
    "mlconf.dbpath = 'http://mlrun-api:8080'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pipeline\"></a>\n",
    "______________________________________________\n",
    "# Create a multi-stage KubeFlow Pipeline from our notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from mlrun import new_project\n",
    "from kubernetes import client as k8sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to execute NetApp volume snap\n",
    "Different function for on prem Netapp and Cloud volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the NetApp snap fucntion once for all functions in a notebook\n",
    "if [ NETAPP_CLOUD_VOLUME ]:\n",
    "    snapfn = code_to_function('snap',project='NetApp',kind='job',filename=\"snap_cv.ipynb\").apply(mount_v3io())\n",
    "    snap_params = {\n",
    "    \"metrics_table\" : metrics_table,\n",
    "    \"NETAPP_MOUNT_PATH\" : NETAPP_MOUNT_PATH,\n",
    "    'MANAGER' : MANAGER,\n",
    "    'svm' : svm,\n",
    "    'email': email,\n",
    "    'password': password ,\n",
    "    'weid': weid,\n",
    "    'volume': volume,\n",
    "    \"APP_DIR\" : APP_DIR\n",
    "       }\n",
    "else:\n",
    "    snapfn = code_to_function('snap',project='NetApp',kind='job',filename=\"snapshot.ipynb\").apply(mount_v3io())\n",
    "    snap_params = {\n",
    "    'ontapClusterMgmtHostname': ontapClusterMgmtHostname,\n",
    "    'ontapClusterAdminUsername': ontapClusterAdminUsername,\n",
    "    'ontapClusterAdminPassword': ontapClusterAdminPassword,\n",
    "    \"metrics_table\" : metrics_table,\n",
    "    \"FEATURES_TABLE\":FEATURES_TABLE,\n",
    "    'sourceVolumeName': sourceVolumeName,'NETAPP_SIM' : NETAPP_SIM,\n",
    "    'NETAPP_MOUNT_PATH': NETAPP_MOUNT_PATH}\n",
    "\n",
    "#.apply(mount_pvc('nfsvol', NETAPP_PVC_CLAIM, NETAPP_MOUNT_PATH))\n",
    "snapfn.spec.image = docker_registry + '/netapp/pipeline:latest'\n",
    "snapfn.spec.volume_mounts = [snapfn.spec.volume_mounts[0],netapp_volume_mounts]\n",
    "snapfn.spec.volumes = [ snapfn.spec.volumes[0],netapp_volumes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Volume snap parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(snapfn.to_yaml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to explore feature metrics\n",
    "We retrieve the function code from a Git repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "describefn =  code_to_function(project='NetApp',kind='job',name='describe',filename='describe.py').apply(mount_v3io())\n",
    "describefn.spec.image = docker_registry + '/iguazio/netapp'\n",
    "\n",
    "### Mount NetApp volume\n",
    "describefn.spec.volume_mounts = [snapfn.spec.volume_mounts[0],netapp_volume_mounts]\n",
    "describefn.spec.volumes = [ snapfn.spec.volumes[0],netapp_volumes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build functions for model training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep = code_to_function(project='NetApp',kind='job',name='data-prep',filename='data-prep.ipynb').apply(mount_v3io())\n",
    "data_prep.spec.image = docker_registry + '/iguazio/netapp'\n",
    "training  = code_to_function(project='NetApp',kind='job',name='training',filename='training.ipynb').apply(mount_v3io())\n",
    "training.spec.image = docker_registry + '/iguazio/netapp'\n",
    "deploy_inference = code_to_function(project='NetApp',kind='job',name='training',filename='deploy-inference-function.ipynb').apply(mount_v3io())\n",
    "deploy_inference.spec.image = docker_registry + '/iguazio/netapp'\n",
    "\n",
    "deploy_features = code_to_function(project='NetApp',kind='job',name='training',filename='deploy-features-function.ipynb').apply(mount_v3io())\n",
    "deploy_features.spec.image = docker_registry + '/iguazio/netapp'\n",
    "\n",
    "### Mount NetApp volume\n",
    "data_prep.spec.volume_mounts = [snapfn.spec.volume_mounts[0],netapp_volume_mounts]\n",
    "data_prep.spec.volumes = [ snapfn.spec.volumes[0],netapp_volumes]\n",
    "\n",
    "training.spec.volume_mounts = [snapfn.spec.volume_mounts[0],netapp_volume_mounts]\n",
    "training.spec.volumes = [ snapfn.spec.volumes[0],netapp_volumes]\n",
    "\n",
    "deploy_inference.spec.volume_mounts = [snapfn.spec.volume_mounts[0],netapp_volume_mounts]\n",
    "deploy_inference.spec.volumes = [ snapfn.spec.volumes[0],netapp_volumes]\n",
    "\n",
    "deploy_features.spec.volume_mounts = [snapfn.spec.volume_mounts[0],netapp_volume_mounts]\n",
    "deploy_features.spec.volumes = [ snapfn.spec.volumes[0],netapp_volumes]\n",
    "\n",
    "#.apply(mount_v3io())\n",
    "#.apply(mount_v3io(name='bigdata', remote='bigdata/',mount_path='/v3io/bigdata'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(training.to_yaml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={   \"FEATURES_TABLE\":FEATURES_TABLE,\n",
    "           \"SAVE_TO\" : SAVE_TO,\n",
    "           \"metrics_table\" : metrics_table,\n",
    "           'FROM_TSDB': 0,\n",
    "           'PREDICTIONS_TABLE': PREDICTIONS_TABLE,\n",
    "           'TRAIN_ON_LAST': '1d',\n",
    "           'TRAIN_SIZE':0.7,\n",
    "           'NUMBER_OF_SHARDS' : 4,\n",
    "           'MODEL_FILENAME' : 'netops.v3.model.pickle',\n",
    "           'APP_DIR' : APP_DIR,\n",
    "           'FUNCTION_NAME' : 'netops-inference',\n",
    "           'PROJECT_NAME' : 'netops',\n",
    "           'NETAPP_SIM' : NETAPP_SIM,\n",
    "           'NETAPP_MOUNT_PATH': NETAPP_MOUNT_PATH,\n",
    "           'NETAPP_PVC_CLAIM' : NETAPP_PVC_CLAIM,\n",
    "           'IGZ_CONTAINER_PATH' : IGZ_CONTAINER_PATH,\n",
    "           'IGZ_MOUNT_PATH' : IGZ_MOUNT_PATH\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**define a 4 step workflow with hyper-params**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='NetOps trainign pipeline with NetApp volume cloning',\n",
    "    description='snap volume before training '\n",
    ")\n",
    "def xgb_pipeline(\n",
    "   ontapClusterMgmtHostname = '',\n",
    "   ontapClusterAdminUsername = '',\n",
    "   ontapClusterAdminPassword = '',\n",
    "   sourceVolumeName = ''\n",
    "):\n",
    "    snap = snapfn.as_step(NewTask(handler='handler',params=snap_params),\n",
    "                            name='NetApp_Cloud_Volume_Snapshot',outputs=['snapVolumeDetails','training_parquet_file']).apply(mount_v3io())\n",
    "    \n",
    "    describe = describefn.as_step(name='describe',handler=\"describe\",params={\"key\": \"summary\", \"label_column\": \"is_error\", 'class_labels': [0, 1]},\n",
    "                            inputs={\"table\": snap.outputs['training_parquet_file']},\n",
    "                            out_path=artifacts_path).apply(mount_v3io()).after(snap)\n",
    "    \n",
    "    prep = data_prep.as_step(name='data-prep', handler='handler',params=params,\n",
    "                          inputs = {'DATA_DIR': snap.outputs['snapVolumeDetails']} ,\n",
    "                          out_path=artifacts_path).apply(mount_v3io()).after(snap)\n",
    "    \n",
    "    deployfeatures = deploy_features.as_step(name='deploy-features-function', handler='handler',params=params,\n",
    "                          inputs = {'DATA_DIR': NETAPP_MOUNT_PATH} ,\n",
    "                          out_path=artifacts_path).apply(mount_v3io()).after(snap)\n",
    "    \n",
    "    train = training.as_step(name='xgb_train', handler='handler',params=params,\n",
    "                            out_path=artifacts_path).apply(mount_v3io()).after(prep)\n",
    "\n",
    "    \n",
    "    deploy = deploy_inference.as_step(name='deploy-model', handler='handler',params=params,                        \n",
    "                       out_path=artifacts_path).apply(mount_v3io()).after(train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a KubeFlow client and submit the pipeline with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debug generate the pipeline dsl\n",
    "kfp.compiler.Compiler().compile(xgb_pipeline, 'mlrunpipe.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client(namespace='default-tenant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"https://dashboard.default-tenant.app.mdl0216.iguazio-cd1.com/pipelines/#/experiments/details/fa75ef87-733d-4cc9-ae86-1217311a5062\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"https://dashboard.default-tenant.app.mdl0216.iguazio-cd1.com/pipelines/#/runs/details/f9c8e2fd-f787-47f9-a1d0-7b9b84887cc6\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arguments={'ontapClusterMgmtHostname': ontapClusterMgmtHostname,\n",
    "           'ontapClusterAdminUsername': ontapClusterAdminUsername,\n",
    "           'ontapClusterAdminPassword':ontapClusterAdminPassword,\n",
    "           'sourceVolumeName': sourceVolumeName\n",
    "            }\n",
    "run_result = client.create_run_from_pipeline_func(xgb_pipeline, arguments, experiment_name='NetAppXGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[back to top](#top)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
